API - Database (alpha)
=========================

This is the alpha version of database management system.
If you have trouble, you can ask for help on `tensorlayer@gmail.com <tensorlayer@gmail.com>`_ .


Why Database
----------------

TensorLayer is designed for real world production, capable of large scale machine learning applications.
TensorLayer database is introduced to address many data management challenges in the large scale machine learning projects, such as:

1. How to find the training data from the enterprise data warehouse.
2. How to load the datasets  so large that are  beyond the storage limitation of one computer.
3. How can we manage different models with version control, and compare them easily.
4. How to automate the whole process for training, evaluation and deployment of machine learning models.

In TensorLayer system, we introduce the database technology to address the challenges above.

TensorDB is designed by following three principles.

Everything is Data
^^^^^^^^^^^^^^^^^^

TensorDB is a data warehouse that stores and captures the whole machine learning development process. The data inside tensordb can be catagloried as:

1. Dataset: This includes all the data for training, validation and prediction. The labels can be manually specified or generated by model prediction.
2. Model architecture: TensorDB includes a repo that stores the different model architecture, enable user to reuse the many model development work.
3. Model parameters: This database stores all the model parameters of each epoch in the training step.
4. Tasks: All the computation tasks are divided  into several small tasks. Each task contains the necessary information such as hyper-parameters for  training or validation. For a training task, typical information includes training data , the model parameter, the model architecture, how many epochs the training task has. Validation, testing and inference are aslo supported by the task system.
5. Loggings: The logs store all the metric of each machine learning model, such as the time stamp, step time and accuracy of each batch or epoch.

TensorDB in principle is a keyword based search engine. Each model, parameter, or training data are assigned with many tags.
The storage system organize data in two layers. The index layer stores all the tags and reference to the blob storage. The index layer is implemented based on NoSQL document database such as MongoDB. The blob layber stores  videos, medical images or label masks in large chunk size, which is usually implemented based upon a  file system. The open source implementation of TensorDB is based on MongoDB. The blob system is based on the gridfs while the indexes are stored as documents.


Everything is identified by Query
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Within Our database framework, any entity within the data warehouse, such as the data, model or tasks are specified by the database query language.
As a reference,  the query is more space efficient for storage  and it can specify multiple objects in a concise way.
Another advantage of such a design is to enable a highly flexible software system.
data, model architecture and training managers are interchangeable.
Many system can be implemented by simply rewire different components, with many new applications can be implemented just by update the query without modification of any applicaition code.

An pulling based Stream processing pipeline.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For large training dataset, we provides a stream interface, which in theory  supports unlimited large dataset.
A stream interface, implemented as a python generators,  keeps on generation the new data during training.
When using the stream interface, the idea of epoch does not apply anymore, instead, we specify the batch size and image a epoch will have a fixed large number of steps.

Many techniques are introduced behind the stream interface for performance optimization.
The stream interface is  based on the database cursor technology.
For every data query, only the cursors are returned immediately, not the actual query results.
The actual data are loaded later when the generators are evaluated.

The data loading is further optimized in many ways:

1. Data are compressed and decompressed,
2. The data are loaded in bulk model to further optimize the IO traffic
3. The data argumentation or random sampling are computed on the fly, only after the data are loaded into the local computer memory.
4. We also introduced simple cache system that stores the recent blob data.

Based on the stream interface, a continuos machine learning system can be easily implemented.
On a distributed system, the model training, validation and deployment can be run by different computing node which are all running continuously.
The trainer  keeps on optimizing the models with new added data, the evaluation node keeps on evaluating the recent generated models and the deployment system keeps pulling the best models from the our database warehouse for application.

Preparation
--------------

In principle, the database can be implemented by any document oriented NoSQL database system.
The existing implementation is based on MongoDB.
Further implementation on other database will be released depends on the progress.
It will be straightforward to port our database system to Google Cloud, AWS and Azure.
The following tutorials are based on the MongoDB implementation.

Install and run MongoDB
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The installation instruction of MongoDB can be found at
`MongoDB Docs <https://docs.MongoDB.com/manual/installation/>`_
there are also managed MongoDB service from amazon or gcp, such as the mongo atlas from MongoDB
User can also user docker, which is a powerful tool for `deploy software <https://hub.docker.com/_/mongo/>`_ .
After install MongoDB, a MongoDB management tool with graphic user interface will be extremely valuale.
Users can install the Studio3T(MongoChef), which is powerful user interface tool for MongoDB and it is free for none commerical usage
`studio3t <https://studio3t.com/>`_

Examples
----------

Connect to the database
^^^^^^^^^^^^^^^^^^^^^^^^^

Similar with MongoDB management tools, IP and port number are required for connecting the database.
In the following example, we run the MongoDB on local machine, thus the IP is localhost, and 27017 is the default port number of MongoDB.

More importantly, the database instances have a ``project_key`` argument, which link

.. code-block:: python

  db = tl.db.TensorHub(ip='localhost', port=27017, dbname='temp',
        username=None, password='password', project_key='tutorial')

Dataset management
^^^^^^^^^^^^^^^^^^^^

You can save a dataset into database then allow all machines to access it.

.. code-block:: python

  db.save_dataset(dataset=[X_train, y_train, X_test, y_test], dataset_key='mnist', description='this is a tutorial')

Then,

.. code-block:: python

  dataset = db.find_one_dataset('mnist')

.. code-block:: python

  datasets = db.find_all_datasets('mnist')


Model management
^^^^^^^^^^^^^^^^^^^^^^^^^

Event / Logging management
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Save training log

.. code-block:: python

  db.train_log(accuray=0.33)
  db.train_log(accuray=0.44)

Delete logs that match the requirement

.. code-block:: python

  db.del_train_log(accuray=0.33)

Delete all logs

.. code-block:: python

  db.del_train_log()

Workflow / Task management
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Uploads a task

.. code-block:: python

  db.push_task(task_key='mnist', script='example/tutorial_mnist_simple.py', description='simple tutorial')

Finds and runs the latest task.

.. code-block:: python

  db.run_one_task(sess=sess, sort=[("time", pymongo.DESCENDING)])

Delete task

.. code-block:: python

  db.del_task()

TensorHub API
---------------------

.. automodule:: tensorlayer.db

.. autoclass:: TensorHub
   :members:
